{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2101202085 - Winemaker Classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfzpqO9atfK"
      },
      "source": [
        "# Multiclass Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8pjZcxYWZ8Y"
      },
      "source": [
        "We previously created a binary classification model that determined if a piece of fruit was an orange or a grapefruit. There are many problems where binary classification can provide impactful solutions: spam or not spam in an email classifier, hit or hold in a blackjack simulation, buy or not in a stock market analysis. The list is basically endless.\n",
        "\n",
        "There are other cases, however, where we want to make a decision across three or more classes. This is multiclass classification.\n",
        "\n",
        "For many applications, multiclass classification can be broken down into many binary classification problems. These models employ a one-vs-all or one-vs-one strategy to create many binary classification tasks that are then aggregated into a multiclass classification model. Neural networks, decision trees, and k-nearest neighbors models are all capable of performing multiclass classification directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1cjMb2GPREz"
      },
      "source": [
        "## The Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUC2a6L5OqIl"
      },
      "source": [
        "For this unit we are going to use a classic machine learning dataset, the [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set). This is a dataset that was used in 1936 by British biologist and statistician Ronald Fisher to classify iris flowers into one of three species based on four measurements:\n",
        "\n",
        "- The length of the petals\n",
        "- The width of the petals\n",
        "- The length of the sepals (the green petal-looking bits that are found at the base of the petals)\n",
        "- The width of the sepals\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaQgAcKkFWcV"
      },
      "source": [
        "# TASK: Winemaker Identification\n",
        "## Tsabita Al Asshifa Hadi Kusuma (2101202085)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9VTg8uUJfwb"
      },
      "source": [
        "Scikit-learn comes prepackaged with many toy datasets. These can be found in the [`sklearn.datasets` package](https://scikit-learn.org/stable/datasets/index.html). In this exercise we'll be working with the [wine dataset](https://scikit-learn.org/stable/datasets/index.html#wine-dataset).\n",
        "\n",
        "The dataset contains information about the properties of wines produced by three different producers. The grapes that the producers used all come from the same region.\n",
        "\n",
        "The columns are:\n",
        "\n",
        "* alcohol\n",
        "* malic_acid\n",
        "* ash\n",
        "* alcalinity_of_ash\n",
        "* magnesium\n",
        "* total_phenols\n",
        "* flavanoids\n",
        "* nonflavanoid_phenols\n",
        "* proanthocyanins\n",
        "* color_intensity\n",
        "* hue\n",
        "* od280/od315_of_diluted_wines\n",
        "* proline\n",
        "\n",
        "The target column is a 0, 1, or 2. Each number represents a different producer.\n",
        "\n",
        "Your task in this exercise is to create a classifier that can identify the producer based on the wine properties.\n",
        "\n",
        "Use as many code blocks as necessary to examine the data and build and validate your model. Document your process using text blocks and/or comments in your code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c80bh__Ff3Mx"
      },
      "source": [
        "# Your Code Goes Here\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wine_bunch = datasets.load_wine()\n",
        "wine_bunch.keys()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzf1QOgmSoU6"
      },
      "source": [
        "\n",
        "FEATURES = wine_bunch['feature_names']\n",
        "TARGET = 'species'\n",
        "\n",
        "wine_df = pd.DataFrame(wine_bunch['data'], columns=FEATURES)\n",
        "wine_df[TARGET] = wine_bunch['target']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WaJ3dLRSzbF"
      },
      "source": [
        "wine_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NjnaQldDJLl"
      },
      "source": [
        "Let's take a look at a description of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNZ2fI-gDWMF"
      },
      "source": [
        "wine_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPb2c7WZD4P_"
      },
      "source": [
        "wine_df.groupby(TARGET).agg('count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viXOR3AseNmB"
      },
      "source": [
        "## Stratified Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE_6f7eMsiW0"
      },
      "source": [
        "Let's first split off a set of data to use for our final model testing. We can use scikit-learn's `train_test_split` function to do this.\n",
        "\n",
        "Since we have so little data, we'll only hold out 10% of the data for the final test.\n",
        "\n",
        "After we make the split, we can see how many data points we will train off of for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9y3IXRaeYPD"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    wine_df[FEATURES],\n",
        "    wine_df[TARGET],\n",
        "    test_size=0.1,\n",
        "    random_state=45)\n",
        "\n",
        "y_train.groupby(y_train).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtO4hGGCv5rc"
      },
      "source": [
        "y_test.groupby(y_test).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x--twmjowI6D"
      },
      "source": [
        "### Exercise 1: Stratified Train Test Split\n",
        "\n",
        "We risk not holding out a data point for every class if we don't stratify our train test split. Rewrite the split above to create a stratified split. (If you don't remember how, try looking at the [documentation for `train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and finding the argument that can be used to stratify the data.) When you are done, there should be 45 data points for each class in the training data and five data points for each class in the testing data. Print the counts to verify."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzk1dDO-wrWr"
      },
      "source": [
        "#### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ1Cf4YMwutU"
      },
      "source": [
        "# Your Code Goes Here\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    wine_df[FEATURES],\n",
        "    wine_df[TARGET],\n",
        "    test_size=0.2,\n",
        "    stratify=wine_df[TARGET])\n",
        "\n",
        "print(y_train.groupby(y_train).count())\n",
        "print(y_test.groupby(y_test).count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJg9KuFiwv4o"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGLRCakNgXHa"
      },
      "source": [
        "## Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pjS5qdBxaFJ"
      },
      "source": [
        "Another problem we have is that we have very little data to work with. We only had 150 data points in total and are only going to train using 135 of those data points. If we are going to be hyperparameter tuning, we'll need a test and validation holdout, which will leave us very little data to train on.\n",
        "\n",
        "One way to get around this is to use cross-validation. Cross-validation splits the data into a fixed number of tranches and trains on `n-1` of the tranches. Then it calculates a score using the holdout tranche. It does this repeatedly, holding out one tranche of data for each training pass. By looking at the mean of the scores for each training pass, you can get an idea of how well your model performs without having to specify a test dataset.\n",
        "\n",
        "The `cross_val_score` method is used to perform the cross-validation. In the example below, we divide the data into five tranches and get five scores.\n",
        "\n",
        "Since we are cross-validating with a classifier, scikit-learn automatically performs stratified splits for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWZ97teWgZuk"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "estimator = SGDClassifier()\n",
        "\n",
        "scores = cross_val_score(\n",
        "    estimator,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqc-VZMV-2AN"
      },
      "source": [
        "We can now find the mean score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q76bjKa--Vy7"
      },
      "source": [
        "scores.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTHzxEVW-4bN"
      },
      "source": [
        "What does this score represent, though? It turns out that it uses the default scoring method for the classifier that we used. In this case we used the [`SGDClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html), which reports accuracy by default.\n",
        "\n",
        "Also note that the estimator isn't trained after running cross-validation. You can run cross-validation to test different data preprocessing pipelines and hyperparameters. Once you are happy with a specific setup, you'll need to train the model with the chosen pipeline and parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSikKKbKAWuG"
      },
      "source": [
        "### Exercise 2: F1 Scoring\n",
        "\n",
        "What if we wanted to use F1 for our scoring metric instead of accuracy?\n",
        "\n",
        "Run `cross_val_score` on an `SGDClassifier` and get the F1 score. Check out the documentation for [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html), [`make_scorer`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html), and [`f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) for clues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk3hK0WGA5WU"
      },
      "source": [
        "#### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVC1CiX0A6-A"
      },
      "source": [
        "# Your Code Goes here\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wine_bunch = datasets.load_wine()\n",
        "wine_bunch.keys()\n",
        "\n",
        "FEATURES = wine_bunch['feature_names']\n",
        "TARGET = 'species'\n",
        "\n",
        "wine_df = pd.DataFrame(wine_bunch['data'], columns=FEATURES)\n",
        "wine_df[TARGET] = wine_bunch['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    wine_df[FEATURES],\n",
        "    wine_df[TARGET], \n",
        "    test_size=0.1,\n",
        "    stratify=wine_df[TARGET])\n",
        "\n",
        "# Solution Starts Here\n",
        "\n",
        "estimator = SGDClassifier()\n",
        "\n",
        "scores = cross_val_score(\n",
        "    estimator,\n",
        "    X_train[FEATURES],\n",
        "    y_train,\n",
        "    cv=5,\n",
        "    scoring=make_scorer(f1_score, average='micro')\n",
        ")\n",
        "\n",
        "\n",
        "scores.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irNxnD-VA8Pb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74WtdKWkBkAj"
      },
      "source": [
        "# The Model Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg-iH2WuCHRd"
      },
      "source": [
        "Since we are now using cross-validation to train the model, we can use our testing holdout data as a final validation. Let's make that clear by renaming the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnwrWu3fCTe5"
      },
      "source": [
        "X_validation = X_test\n",
        "y_validation = y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9juL1rrCfqK"
      },
      "source": [
        "Now we can work on tuning the model and the model pipeline.\n",
        "\n",
        "Let's first look back at the data going into the model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUCG4eT6DOX5"
      },
      "source": [
        "wine_df[FEATURES].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIN9j51JDw-a"
      },
      "source": [
        "The data is all in the same order of magnitude, but columns like 'sepal length (cm)' are considerably larger than columns like 'petal width (cm)'.\n",
        "\n",
        "We need to perform some preprocessing to get the data into a more uniform shape before feeding it to the model. To do that we'll use the [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), which removes the mean and subtracts the unit variance from each column of data.\n",
        "\n",
        "To use the `StandardScaler`, we create the object, `fit()` the data, and then `transform()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdy8I__jDSnY"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "scaler.fit(wine_df[FEATURES])\n",
        "\n",
        "pd.DataFrame(\n",
        "    scaler.transform(wine_df[FEATURES]),\n",
        "    columns=FEATURES\n",
        ").describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueBrxZcpEm3C"
      },
      "source": [
        "You can see in the output of `describe()` that the data now all has a standard deviation that approaches one.\n",
        "\n",
        "We need to perform this preprocessing to features before training the model and before getting predictions. It can be error-prone to try to remember to do this. To make the task easier, we can create an estimator [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) that applies our transformations and calls our estimator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl0syxvSFO_c"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "estimator = Pipeline(\n",
        "  steps=[\n",
        "    ['scale', StandardScaler()],\n",
        "    ['classifier', SGDClassifier()],\n",
        "  ]\n",
        ")\n",
        "\n",
        "scores = cross_val_score(\n",
        "    estimator,\n",
        "    X_train[FEATURES],\n",
        "    y_train,\n",
        "    cv=5,\n",
        ")\n",
        "\n",
        "scores.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDkGp3ykFYea"
      },
      "source": [
        "Scaling gave us a considerable jump in accuracy score. Hopefully you see similar results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylEPncqoFgYu"
      },
      "source": [
        "### Exercise 3: Final Validation\n",
        "\n",
        "Our accuracy results were pretty good, so we aren't going to do any more hyperparameter tuning in this lab. Before we declare victory, though, we should find the F1 score of our validation data. Using our estimator pipeline, calculate the F1 score for `X_validation`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxBqABGUFyXv"
      },
      "source": [
        "# Your Code Goes Here\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "wine_bunch = datasets.load_wine()\n",
        "wine_bunch.keys()\n",
        "\n",
        "FEATURES = wine_bunch['feature_names']\n",
        "TARGET = 'species'\n",
        "\n",
        "wine_df = pd.DataFrame(wine_bunch['data'], columns=FEATURES)\n",
        "wine_df[TARGET] = wine_bunch['target']\n",
        "\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(\n",
        "    wine_df[FEATURES],\n",
        "    wine_df[TARGET], \n",
        "    test_size=0.1,\n",
        "    stratify=wine_df[TARGET])\n",
        "\n",
        "estimator = Pipeline(\n",
        "  steps=[\n",
        "    ['scale', StandardScaler()],\n",
        "    ['classifier', SGDClassifier()],\n",
        "  ]\n",
        ")\n",
        "\n",
        "scores = cross_val_score(\n",
        "    estimator,\n",
        "    X_train[FEATURES],\n",
        "    y_train,\n",
        "    cv=5,\n",
        "    # We calculated F1 here too. This isn't required.\n",
        "    scoring=make_scorer(f1_score, average='micro')\n",
        ")\n",
        "\n",
        "# Solution Starts Here\n",
        "estimator.fit(X_train, y_train)\n",
        "predictions = estimator.predict(X_validation)\n",
        "f1_score(y_validation, predictions, average='micro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN0937JcuFmY"
      },
      "source": [
        "#importing confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(y_validation, predictions)\n",
        "print('Confusion Matrix\\n')\n",
        "print(confusion)\n",
        "\n",
        "#importing accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_validation, predictions)))\n",
        "\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_validation, predictions, average='micro')))\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_validation, predictions, average='micro')))\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_validation, predictions, average='micro')))\n",
        "\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_validation, predictions, average='macro')))\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_validation, predictions, average='macro')))\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_validation, predictions, average='macro')))\n",
        "\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_validation, predictions, average='weighted')))\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_validation, predictions, average='weighted')))\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_validation, predictions, average='weighted')))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('\\nClassification Report\\n')\n",
        "print(classification_report(y_validation, predictions, target_names=['Class 1', 'Class 2', 'Class 3']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJHBoilPKfTU"
      },
      "source": [
        "---"
      ]
    }
  ]
}